{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    #クラスの数\n",
    "    'metric': n,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbms(X, X_test, y, params):\n",
    "\n",
    "    NFOLDS = 5\n",
    "    folds = KFold(n_splits=NFOLDS, shuffle=True, random_state=12)\n",
    "\n",
    "    preds = pd.DataFrame()\n",
    "    train_preds = pd.DataFrame()\n",
    "\n",
    "    columns = X.columns\n",
    "    splits = folds.split(X, y)\n",
    "\n",
    "    y_preds = np.zeros(X_test.shape[0])\n",
    "    y_oof = np.zeros(X.shape[0])\n",
    "\n",
    "    score = 0\n",
    "    scores = []\n",
    "\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = columns\n",
    "\n",
    "    for fold_n, (train_index, val_index) in enumerate(splits):\n",
    "        # split train, Valid\n",
    "        X_train, X_val = X[columns].iloc[train_index], X[columns].iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        '''\n",
    "        LightGBM\n",
    "        '''\n",
    "        dtrain = lgb.Dataset(X_train, y_train)\n",
    "        dval = lgb.Dataset(X_val, y_val)\n",
    "\n",
    "        clf = lgb.train(params,\n",
    "                        dtrain,\n",
    "                        1000,\n",
    "                        valid_sets=[dtrain, dval],\n",
    "                        categorical_feature=col_category,\n",
    "                        verbose_eval=200,\n",
    "                        early_stopping_rounds=100)\n",
    "\n",
    "        feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "\n",
    "        '''\n",
    "        LightGBM\n",
    "        '''\n",
    "        # stack\n",
    "        y_pred_val = clf.predict(X_val)\n",
    "        y_oof[val_index] = y_pred_val\n",
    "        \n",
    "        \n",
    "        #print(f\"Fold {fold_n + 1} | rmse: {accuracy_score(y_val), y_pred_val))}\")\n",
    "        #score += np.sqrt(mean_squared_error(np.exp(y_val), np.exp(y_pred_val))) / NFOLDS\n",
    "        #scores.append(np.sqrt(mean_squared_error(np.exp(y_val), np.exp(y_pred_val))))\n",
    "\n",
    "        #print(f\"Fold {fold_n + 1} | rmse: {np.sqrt(mean_squared_error(y_val, y_pred_val))}\")\n",
    "\n",
    "        y_preds += clf.predict(X_test) / NFOLDS\n",
    "\n",
    "        train_preds[f\"Fold {fold_n + 1}\"] = clf.predict(X)\n",
    "\n",
    "        preds[f\"Fold {fold_n + 1}\"] = clf.predict(X_test)\n",
    "\n",
    "        del X_train, X_val, y_train, y_val\n",
    "        gc.collect()\n",
    "    #print(f\"\\nMean RSME = {score}\")\n",
    "\n",
    "    return y_oof, y_preds, feature_importances, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X, y)\n",
    "y_preds = np.zeros(X_test.shape[0])\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "  \n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=200, early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_oof[valid_index] = y_pred_valid\n",
    "    print(f\"Fold {fold_n + 1} | AUC: {roc_auc_score(y_valid, y_pred_valid)}\")\n",
    "    \n",
    "    score += roc_auc_score(y_valid, y_pred_valid) / NFOLDS\n",
    "    y_preds += clf.predict(X_test) / NFOLDS\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    gc.collect()\n",
    "    \n",
    "print(f\"\\nMean AUC = {score}\")\n",
    "print(f\"Out of folds AUC = {roc_auc_score(y, y_oof)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ラベル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    " \n",
    "for x in preds:\n",
    "    y_pred.append(np.argmax(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.read_csv(path+'feature_importance/NO13_2_features.csv')\n",
    "feature_importances['average'] = feature_importances[[f'fold_{fold_n + 1}' for fold_n in range(10)]].mean(axis=1)\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.barplot(data=feature_importances.sort_values(by='average', ascending=False).head(50), x='average', y='feature');\n",
    "plt.title('250 TOP feature importance over {} folds average'.format(10))\n",
    "#plt.savefig('TOP250_feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "T = np_utils.to_categorical(T)\n",
    "Using TensorFlow backen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: nan - accuracy: 0.2634 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: nan - accuracy: 0.0987 - val_loss: nan - val_accuracy: 0.0980\n",
      "Test Loss:nan\n",
      "Test Accuracy: 0.09799999743700027\n",
      "time:41.133sec\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#データの読み込みと前処理\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "#kerasでCNN構築\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "#時間計測\n",
    "import time\n",
    " \n",
    " \n",
    "'''\n",
    "データの読み込みと前処理\n",
    "'''\n",
    "#データの読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "#訓練データ\n",
    "X_train = X_train.reshape(60000, 784)#平滑化\n",
    "X_train = X_train.astype('float32')#型を変更\n",
    "X_train /= 255 #0から1.0の範囲に変換\n",
    " \n",
    "#正解ラベル\n",
    "correct = 10\n",
    "y_train = np_utils.to_categorical(y_train, correct)\n",
    " \n",
    "#テストデータ\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test /= 255\n",
    " \n",
    "y_test = np_utils.to_categorical(y_test, correct)\n",
    " \n",
    "'''\n",
    "入力+中間層+出力層の３層NNの構築\n",
    "'''\n",
    " \n",
    "model = Sequential()\n",
    " \n",
    "model.add(Dense(200, input_dim=784, activation='relu'))\n",
    " \n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    " \n",
    "'''\n",
    "学習\n",
    "'''\n",
    "#計測開始\n",
    "startTime = time.time()\n",
    " \n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=100, verbose=1, validation_data=(X_test, y_test))\n",
    " \n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    " \n",
    "#誤り率\n",
    "print('Test Loss:{0:.3f}'.format(score[0]))\n",
    "#正解率\n",
    "print('Test Accuracy:', score[1])\n",
    "#処理にかかった時間\n",
    "print(\"time:{0:.3f}sec\".format(time.time() - startTime))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NFOLDS = 5\n",
    "folds = KFold(n_splits=NFOLDS)\n",
    "\n",
    "columns = X.columns\n",
    "splits = folds.split(X, y)\n",
    "y_preds = pd.DataFrame()\n",
    "\n",
    "y_oof = np.zeros(X.shape[0])\n",
    "score = 0\n",
    "\n",
    "feature_importances = pd.DataFrame()\n",
    "feature_importances['feature'] = columns\n",
    "  \n",
    "for fold_n, (train_index, valid_index) in enumerate(splits):\n",
    "    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "\n",
    "    clf = lgb.train(params, \n",
    "                    dtrain, \n",
    "                    1000, \n",
    "                    valid_sets = [dtrain, dvalid], \n",
    "                    verbose_eval=200, \n",
    "                    early_stopping_rounds=500)\n",
    "    \n",
    "    feature_importances[f'fold_{fold_n + 1}'] = clf.feature_importance()\n",
    "    \n",
    "    y_pred_valid = clf.predict(X_valid)\n",
    "    y_oof[valid_index] = np.argmax(y_pred_valid)\n",
    "    \n",
    "    print(f\"Fold {fold_n + 1} | Accuracy: {accuracy_score(np.argmax(y_valid, axis=1),np.argmax(y_pred_valid, axis=1))}\")\n",
    "    \n",
    "    y_preds[f'fold_{fold_n + 1}'] = np.argmax(clf.predict(X_test), axis=1)\n",
    "    \n",
    "    del X_train, X_valid, y_train, y_valid\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
